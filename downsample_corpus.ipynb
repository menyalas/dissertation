{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99595636",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f08414e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.sparse import coo_matrix, csr_matrix, csc_matrix, lil_matrix, vstack\n",
    "from scipy.stats import gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb7912f",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81a5757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Read in vocabulary from file.\n",
    "def get_vocab(vocab_fn, ignore_case):\n",
    "    vocab = []\n",
    "    vocab_index = {}\n",
    "    for i, line in enumerate(open(vocab_fn, mode='r', encoding='utf-8')):\n",
    "        term = line.strip()\n",
    "        if ignore_case:\n",
    "            term = term.lower()\n",
    "        vocab.append(term)\n",
    "        vocab_index[term] = i\n",
    "    return vocab, vocab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45f9e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# From input corpus in_tsv and the index of working vocabulary vocab_index\n",
    "# construct:\n",
    "#   authors: working list of authors\n",
    "#   author_doc_ids: mapping of authors to document ids\n",
    "#   doc_term_matrix: document-term matrix\n",
    "def process_corpus(in_tsv, vocab_index, ignore_case, verbose):\n",
    "    vocab_size = len(vocab_index)\n",
    "    authors_by_doc = []\n",
    "    doc_vectors = []\n",
    "    n_lines = sum(1 for line in open(in_tsv))\n",
    "    reader = open(in_tsv, mode='r', encoding='utf-8')\n",
    "    for i, line in enumerate(reader):\n",
    "        if verbose and i and i % 1000 == 0:\n",
    "            print('{}/{}'.format(i, n_lines), file=sys.stderr)\n",
    "        fields = line.strip().split('\\t')\n",
    "        authors_by_doc.append(fields[1])\n",
    "        vector = lil_matrix((1, vocab_size))\n",
    "        tokens = fields[2].split()\n",
    "        if ignore_case:\n",
    "            tokens = [t.lower() for t in tokens]\n",
    "        term_counts = Counter(tokens)\n",
    "        for term in term_counts:\n",
    "            if term in vocab_index:\n",
    "                col = vocab_index[term]\n",
    "                vector[0, col] = term_counts[term]\n",
    "        doc_vectors.append(vector)\n",
    "    doc_term_matrix = vstack(doc_vectors, format='csr')\n",
    "    authors = sorted(list(set(authors_by_doc)))\n",
    "    author_index = {author: i for i, author in enumerate(authors)}\n",
    "    author_doc_ids = {author: [] for author in authors}\n",
    "    for i, a in enumerate(authors_by_doc):\n",
    "        author_doc_ids[a].append(i)\n",
    "    return authors, author_index, author_doc_ids, doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "333d79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# Construct author-term matrix from document-term matrix.\n",
    "def get_author_term_matrix(authors, author_doc_ids, doc_term_matrix):\n",
    "    author_vectors = [csr_matrix(doc_term_matrix[doc_ids].sum(axis=0)) for\n",
    "                      doc_ids in author_doc_ids.values()]\n",
    "    author_term_matrix = vstack(author_vectors, format='csc')\n",
    "    return author_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fba9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in Step 4\n",
    "\n",
    "# Estimate gamma parameters k, theta using method of moments\n",
    "def get_gamma_parameters(author_term_freqs):\n",
    "    term_means = np.mean(author_term_freqs, axis=0).getA1()\n",
    "    author_term_freqs = author_term_freqs.toarray() # Added to resolve error\n",
    "    term_vars = np.var(author_term_freqs, axis=0, ddof=1)#.getA1() Dropped this to resolve error\n",
    "    ks = np.divide(np.square(term_means), term_vars)\n",
    "    thetas = np.divide(term_vars, term_means)\n",
    "    return ks, thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "286a6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "# Build author-term stop weight matrix for given author-term matrix and\n",
    "# probability threshold.\n",
    "def get_stop_weights(author_term_matrix, threshold):\n",
    "    n_authors, n_terms = author_term_matrix.shape\n",
    "    weights = lil_matrix((n_authors, n_terms))\n",
    "    author_term_freqs = author_term_matrix / author_term_matrix.sum(axis=1)\n",
    "    ks, thetas = get_gamma_parameters(author_term_freqs)\n",
    "\n",
    "    # Assign probabilities of deletion for each partition-feature pair\n",
    "    author_term_freqs = csc_matrix(author_term_freqs) # Added to convert non-indexable COO to indexable CSC format \n",
    "    \n",
    "    for term_id in range(n_terms):\n",
    "        author_freqs = author_term_freqs[:, term_id].toarray().flatten() # Added to replace the line below\n",
    "        #author_freqs = author_term_freqs[:, term_id].getA1() # Replaced wth line above\n",
    "        freq_threshold = gamma.ppf(1-threshold, ks[term_id],\n",
    "                                   scale=thetas[term_id])\n",
    "        for author_id, freq in enumerate(author_freqs):\n",
    "            if freq_threshold < freq:\n",
    "                weights[author_id, term_id] = 1 - freq_threshold / freq\n",
    "    weights = weights.tocsr()\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb17822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5\n",
    "# Downsample input according to the author-term stop weights matrix.\n",
    "def downsample(in_tsv, vocab_index, doc_term_matrix, author_index, # changed from document_term_matrix\n",
    "               weights, out_tsv, min_tokens, ignore_case, verbose): # changed from author_term_weights\n",
    "    writer = open(out_tsv, mode='w', encoding='utf-8')\n",
    "    n_docs = doc_term_matrix.shape[0] # Changed from document_term_matrix\n",
    "    for doc_id, line in enumerate(open(in_tsv, mode='r', encoding='utf-8')):\n",
    "        if verbose and doc_id and doc_id % 1000 == 0:\n",
    "            print('{}/{}'.format(doc_id, n_docs), file=sys.stderr)\n",
    "        fields = line.strip().split('\\t')\n",
    "        author = fields[1]\n",
    "        author_id = author_index[author]\n",
    "        tokens = fields[2].split()\n",
    "        # Filter tokens based on working vocabulary\n",
    "        if ignore_case:\n",
    "            tokens = [t for t in tokens if t.lower() in vocab_index]\n",
    "            term_ids = np.array([vocab_index[t.lower()] for t in tokens])\n",
    "        else:\n",
    "            tokens = [t for t in tokens if t in vocab_index]\n",
    "            term_ids = np.array([vocab_index[t] for t in tokens])\n",
    "\n",
    "        # Construct token mask for curation\n",
    "        term_stop_rates = weights.getrow(author_id) # changed from author_term_weights\n",
    "        term_stop_rates = term_stop_rates.toarray().ravel()\n",
    "        token_keep_rates = [1-term_stop_rates[i] for i in term_ids]\n",
    "        token_mask = np.random.binomial(1, token_keep_rates)\n",
    "        n_kept = token_mask.sum()\n",
    "\n",
    "        # Write document if it has at least min_tokens tokens\n",
    "        if n_kept >= min_tokens:\n",
    "            token_mask = token_mask.astype(bool)\n",
    "            stopped_text = ' '.join(np.array(tokens)[token_mask])\n",
    "            writer.write('{}\\t{}\\t{}\\n'.format(fields[0], fields[1],\n",
    "                                               stopped_text))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d4e4f",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0950ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "vocab = get_vocab(\"20240909_PhD_DiaryDictionary.txt\", ignore_case=True)\n",
    "vocab_index = vocab[1]\n",
    "#vocab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef1a3288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "corpus = process_corpus(\"20240909_PhD_DiaryCorpus.tsv\", vocab_index, ignore_case=True, verbose=False)\n",
    "authors = corpus[0]\n",
    "author_index = corpus[1]\n",
    "author_doc_ids = corpus[2]\n",
    "doc_term_matrix = corpus[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67010572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "author_term_matrix = get_author_term_matrix(authors, author_doc_ids, doc_term_matrix)\n",
    "#print(author_term_matrix)\n",
    "#author_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "140190d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "weights = get_stop_weights(author_term_matrix, threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d14cfb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "downsample(\"20240909_PhD_DiaryCorpus.tsv\", vocab_index, doc_term_matrix, author_index,\n",
    "               weights, \"20240911_PhD_DiaryCorpus.tsv\", min_tokens=1, ignore_case=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78241925",
   "metadata": {},
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fece58f5",
   "metadata": {},
   "source": [
    "The get_stop_weights function produces a sparse matrix (COO) called author_term_freqs, which the get_gamma_parameters function calls to calculate variance; however, SciPy does not have a variance function for sparse matrix. Trying out different ways to solve. Options include: 1) converting the sparse matrix to a numpy array, then running just the variance on that; 2) doing the same but before calculating the means, given the advice in the Scipy documentation to convert before applying numpy calculations; 3) using a custom function to calculate variance directly on the sparse arrray. ChatGPT was used to explore possible solutions to this problem. The variance function was obtained from https://gist.github.com/sumartoyo/edba2eee645457a98fdf046e1b4297e4. The following Scipy and Numpy documentation were also used. Solution 1 and 2 produced the same results while solution 3 produced different figures as shown below. This might be because the function could not accommodate the degrees of freedom parameter. To minimize the amount of meddling with the original code, I will stick with solution 1.\n",
    "<ul>\n",
    "    <li>https://docs.scipy.org/doc/scipy-1.12.0/reference/sparse.html#sparse-matrix-classes</li>\n",
    "<li>https://docs.scipy.org/doc/scipy-1.12.0/reference/generated/scipy.sparse.coo_matrix.mean.html#scipy.sparse.coo_matrix.mean</li>\n",
    "    <li>https://numpy.org/devdocs/reference/generated/numpy.matrix.mean.html#numpy.matrix.mean</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "80def328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_authors, n_terms = author_term_matrix.shape\n",
    "#weights = lil_matrix((n_authors, n_terms))\n",
    "#author_term_freqs = author_term_matrix / author_term_matrix.sum(axis=1)\n",
    "#ks, thetas = get_gamma_parameters(author_term_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f6eadaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25 0.25 0.25 ... 0.25 0.25 0.25] [1.46563095e-04 5.80214679e-05 5.80214679e-05 ... 5.80214679e-05\n",
      " 5.80214679e-05 3.68935621e-04]\n"
     ]
    }
   ],
   "source": [
    "# Solution 1\n",
    "\n",
    "#def get_gamma_parameters(author_term_freqs):\n",
    "#    term_means = np.mean(author_term_freqs, axis=0).getA1()\n",
    "#    author_term_freqs = author_term_freqs.toarray()\n",
    "#    term_vars = np.var(author_term_freqs, axis=0, ddof=1)\n",
    "#    ks = np.divide(np.square(term_means), term_vars)\n",
    "#    thetas = np.divide(term_vars, term_means)\n",
    "#    return ks, thetas\n",
    "\n",
    "#n_authors, n_terms = author_term_matrix.shape\n",
    "#weights = lil_matrix((n_authors, n_terms))\n",
    "#author_term_freqs = author_term_matrix / author_term_matrix.sum(axis=1)\n",
    "#ks, thetas = get_gamma_parameters(author_term_freqs)\n",
    "\n",
    "#print(ks, thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6a4ffc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25 0.25 0.25 ... 0.25 0.25 0.25] [1.46563095e-04 5.80214679e-05 5.80214679e-05 ... 5.80214679e-05\n",
      " 5.80214679e-05 3.68935621e-04]\n"
     ]
    }
   ],
   "source": [
    "# Solution 2\n",
    "\n",
    "#def get_gamma_parameters(author_term_freqs):\n",
    "#    author_term_freqs = author_term_freqs.toarray()\n",
    "#    term_means = np.mean(author_term_freqs, axis=0)\n",
    "#    term_vars = np.var(author_term_freqs, axis=0, ddof=1)\n",
    "#    ks = np.divide(np.square(term_means), term_vars)\n",
    "#    thetas = np.divide(term_vars, term_means)\n",
    "#    return ks, thetas\n",
    "\n",
    "#n_authors, n_terms = author_term_matrix.shape\n",
    "#weights = lil_matrix((n_authors, n_terms))\n",
    "#author_term_freqs = author_term_matrix / author_term_matrix.sum(axis=1)\n",
    "#ks, thetas = get_gamma_parameters(author_term_freqs)\n",
    "\n",
    "#print(ks, thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b4bcb3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33333333 0.33333333 0.33333333 ... 0.33333333 0.33333333 0.33333333] [1.09922322e-04 4.35161010e-05 4.35161010e-05 ... 4.35161010e-05\n",
      " 4.35161010e-05 2.76701716e-04]\n"
     ]
    }
   ],
   "source": [
    "# Solution 3\n",
    "\n",
    "#def get_gamma_parameters(author_term_freqs):\n",
    "#    term_means = np.mean(author_term_freqs, axis=0).getA1()\n",
    "#    term_vars = vars(author_term_freqs, axis=0).getA1()\n",
    "#    ks = np.divide(np.square(term_means), term_vars)\n",
    "#    thetas = np.divide(term_vars, term_means)\n",
    "#    return ks, thetas\n",
    "\n",
    "#def vars(a, axis=0):\n",
    "#    \"\"\" Variance of sparse matrix a\n",
    "#    var = mean(a**2) - mean(a)**2\n",
    "#    \"\"\"\n",
    "#    a_squared = a.copy()\n",
    "#    a_squared.data **= 2\n",
    "#    return a_squared.mean(axis) - np.square(a.mean(axis))\n",
    "\n",
    "#n_authors, n_terms = author_term_matrix.shape\n",
    "#weights = lil_matrix((n_authors, n_terms))\n",
    "#author_term_freqs = author_term_matrix / author_term_matrix.sum(axis=1)\n",
    "#ks, thetas = get_gamma_parameters(author_term_freqs)\n",
    "\n",
    "#print(ks, thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6c51c",
   "metadata": {},
   "source": [
    "Another hitch with the get stop weights function occurred with the creation of the author_freqs object, which tried to index over a coo_matrix, which cannot be done. When I converted this to csc_matric, the getA1() method did not work. I used ChatGPT to help me resolve this problem but the result yields an empty matrix. This might be because the threshhold is too low. Gradually increased until the matrix was no longer empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7d253743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x4887 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3842 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_authors, n_terms = author_term_matrix.shape\n",
    "weights = lil_matrix((n_authors, n_terms))\n",
    "author_term_freqs = author_term_matrix / author_term_matrix.sum(axis=1)\n",
    "ks, thetas = get_gamma_parameters(author_term_freqs)\n",
    "\n",
    "# Assign probabilities of deletion for each partition-feature pair\n",
    "author_term_freqs = csc_matrix(author_term_freqs) # Added to convert non-indexable COO to indexable CSC format \n",
    "    \n",
    "for term_id in range(n_terms):\n",
    "    author_freqs = author_term_freqs[:, term_id].toarray().flatten() # Added to replace the line below\n",
    "    #author_freqs = author_term_freqs[:, term_id].getA1() # Replaced wth line above\n",
    "    freq_threshold = gamma.ppf(1-.07, ks[term_id],\n",
    "                                   scale=thetas[term_id])\n",
    "    for author_id, freq in enumerate(author_freqs):\n",
    "        if freq_threshold < freq:\n",
    "            weights[author_id, term_id] = 1 - freq_threshold / freq\n",
    "weights = weights.tocsr()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2a829755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x4887 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 904 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diagnostics\n",
    "\n",
    "#author_freqs\n",
    "weights.getrow(3) # Non-zero values -- Housewife (1568), Labourer (218), Businessman (1152), Lady (904)\n",
    "#print(author_term_freqs.getcol(0))\n",
    "#author_term_freqs.shape\n",
    "#type(weights)\n",
    "#weights.get_shape()\n",
    "#print(weights.getrow(0))\n",
    "#weights.getcol(0)\n",
    "#weights.count_nonzero()\n",
    "#print(weights)\n",
    "#author_term_matrix # CSC 4x4887, 6521\n",
    "#n_authors, n_terms # (4, 4887)\n",
    "#weights # List of Lists format 0 to CSR 0\n",
    "#author_term_freqs # COO 4x4887, 6521 to CSC 4x4887, 6521 (alt CSR 4x4887, 6521)\n",
    "#len(ks) # 4887\n",
    "#len(thetas) # 4887\n",
    "#len(author_freqs) # 4\n",
    "#freq_threshold # 0.00044645494832863414"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905dda81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
