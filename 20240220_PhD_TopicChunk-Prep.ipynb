{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model Chunk Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3785 entries, 0 to 3784\n",
      "Data columns (total 33 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        3785 non-null   int64  \n",
      " 1   docauthorid       3785 non-null   object \n",
      " 2   docauthorname     3785 non-null   object \n",
      " 3   docid             3785 non-null   object \n",
      " 4   sourcetitle       3785 non-null   object \n",
      " 5   docyear           3741 non-null   float64\n",
      " 6   docmonth          2824 non-null   float64\n",
      " 7   docday            2215 non-null   float64\n",
      " 8   authorgender      3785 non-null   object \n",
      " 9   agewriting        2817 non-null   float64\n",
      " 10  birthyear         2861 non-null   float64\n",
      " 11  deathyear         2849 non-null   float64\n",
      " 12  religionNew       2501 non-null   object \n",
      " 13  relMin            3198 non-null   object \n",
      " 14  nationalOrigin    3780 non-null   object \n",
      " 15  britishEmpire_EU  3773 non-null   object \n",
      " 16  translated        3785 non-null   bool   \n",
      " 17  authorLocation    3785 non-null   object \n",
      " 18  socialClass       3177 non-null   object \n",
      " 19  A                 3177 non-null   object \n",
      " 20  I                 3177 non-null   object \n",
      " 21  CCP               3177 non-null   object \n",
      " 22  Unknown           3177 non-null   object \n",
      " 23  wageLabour        3177 non-null   object \n",
      " 24  publicLetter      659 non-null    object \n",
      " 25  chunk             3785 non-null   object \n",
      " 26  sequence          3785 non-null   int64  \n",
      " 27  scoreNeg          3785 non-null   float64\n",
      " 28  scorePos          3785 non-null   float64\n",
      " 29  scoreNeu          3785 non-null   float64\n",
      " 30  scoreCompound     3785 non-null   float64\n",
      " 31  chunks            3785 non-null   int64  \n",
      " 32  position          3785 non-null   float64\n",
      "dtypes: bool(1), float64(11), int64(3), object(18)\n",
      "memory usage: 950.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Sentence Data\n",
    "df = pd.read_csv(\"20240220_PhD_Data4TopicModel-Chunk.csv\") \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['docID-AT',\n",
       " 'docauthorid',\n",
       " 'docauthorname',\n",
       " 'docid',\n",
       " 'sourcetitle',\n",
       " 'docyear',\n",
       " 'docmonth',\n",
       " 'docday',\n",
       " 'authorgender',\n",
       " 'agewriting',\n",
       " 'birthyear',\n",
       " 'deathyear',\n",
       " 'religionNew',\n",
       " 'relMin',\n",
       " 'nationalOrigin',\n",
       " 'britishEmpire_EU',\n",
       " 'translated',\n",
       " 'authorLocation',\n",
       " 'socialClass',\n",
       " 'A',\n",
       " 'I',\n",
       " 'CCP',\n",
       " 'Unknown',\n",
       " 'wageLabour',\n",
       " 'publicLetter',\n",
       " 'text',\n",
       " 'sequence',\n",
       " 'scoreNeg',\n",
       " 'scorePos',\n",
       " 'scoreNeu',\n",
       " 'scoreCompound',\n",
       " 'chunks',\n",
       " 'position']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change column name to \"docID-AT\"\n",
    "df = df.rename(columns={'Unnamed: 0':'docID-AT'})\n",
    "df = df.rename(columns={'chunk':'text'})\n",
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"TRINIDAD On Train from Steubenville, Ohio, to Cincinnati. Nov 30, 1872. My Darling Sister Justina: How interestedly you, Sister M Louis and myself read Eugénie de Guérin's Journal and her daily anxieties to save her brother from being a spiritual outcast! This Journal which I propose keeping for you will deal with incidents occurring on my journey to Trinidad and happenings in that far-off land to which I am consigned. The Journal will begin with the first act. Here is Mother Josephine's letter: Mt St Vincent, O, Nov 27, 1872. Sister Blandina, Steubenville, O My Dear Child: You are missioned to Trinidad. You will leave Cincinnati Wednesday and alone. Mother Regina will attend to your needs. Devotedly, Mother Josephine. This letter thrilled us both. I was delighted to make the sacrifice, and you were hiding your feelings that I might not lose any merit. Neither of us could find Trinidad on the map except in the island of Cuba. So we concluded that Cuba was my destination. I was to leave Steubenville quietly so that none of my obstreperous pupils might cause the incoming teacher\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below adapated from https://medium.com/analytics-vidhya/topic-modeling-using-gensim-lda-in-python-48eaa2344920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"TRINIDAD On Train from Steubenville, Ohio, to Cincinnati. Nov 30, 1872. My Darling Sister Justina: How interestedly you, Sister M Louis and myself read Eugénie de Guérin's Journal and her daily anxieties to save her brother from being a spiritual outcast! This Journal which I propose keeping for you will deal with incidents occurring on my journey to Trinidad and happenings in that far-off land to which I am consigned. The Journal will begin with the first act. Here is Mother Josephine's letter: Mt St Vincent, O, Nov 27, 1872. Sister Blandina, Steubenville, O My Dear Child: You are missioned to Trinidad. You will leave Cincinnati Wednesday and alone. Mother Regina will attend to your needs. Devotedly, Mother Josephine. This letter thrilled us both. I was delighted to make the sacrifice, and you were hiding your feelings that I might not lose any merit. Neither of us could find Trinidad on the map except in the island of Cuba. So we concluded that Cuba was my destination. I was to leave Steubenville quietly so that none of my obstreperous pupils might cause the incoming teacher\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert values in text to list of strings (objects)\n",
    "data = df.text.values.tolist()\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['trinidad', 'train', 'from', 'steubenville', 'ohio', 'cincinnati', 'nov', 'darling', 'sister', 'justina', 'how', 'interestedly', 'you', 'sister', 'louis', 'and', 'myself', 'read', 'eugenie', 'guerin', 'journal', 'and', 'her', 'daily', 'anxieties', 'save', 'her', 'brother', 'from', 'being', 'spiritual', 'outcast', 'this', 'journal', 'which', 'propose', 'keeping', 'for', 'you', 'will', 'deal', 'with', 'incidents', 'occurring', 'journey', 'trinidad', 'and', 'happenings', 'that', 'far', 'off', 'land', 'which', 'consigned', 'the', 'journal', 'will', 'begin', 'with', 'the', 'first', 'act', 'here', 'mother', 'josephine', 'letter', 'vincent', 'nov', 'sister', 'blandina', 'steubenville', 'dear', 'child', 'you', 'are', 'missioned', 'trinidad', 'you', 'will', 'leave', 'cincinnati', 'wednesday', 'and', 'alone', 'mother', 'regina', 'will', 'attend', 'your', 'needs', 'devotedly', 'mother', 'josephine', 'this', 'letter', 'thrilled', 'both', 'was', 'delighted', 'make', 'the', 'sacrifice', 'and', 'you', 'were', 'hiding', 'your', 'feelings', 'that', 'might', 'not', 'lose', 'any', 'merit', 'neither', 'could', 'find', 'trinidad', 'the', 'map', 'except', 'the', 'island', 'cuba', 'concluded', 'that', 'cuba', 'was', 'destination', 'was', 'leave', 'steubenville', 'quietly', 'that', 'none', 'obstreperous', 'pupils', 'might', 'cause', 'the', 'incoming', 'teacher']]\n"
     ]
    }
   ],
   "source": [
    "# Define function to convert texts into a list of lowercase tokens\n",
    "def text_to_words(texts):\n",
    "    for text in texts:\n",
    "        yield(gensim.utils.simple_preprocess(str(text), \n",
    "                                             deacc=True, # removes accents\n",
    "                                             min_len=3))   # removes tokens shorter than three characters\n",
    "\n",
    "data_words = list(text_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absolutely_necessary', 'albany_buffalo', 'alexander_robb', 'allen_ransome', 'ancient_civilization', 'angels_guard', 'annual_pass', 'archbishop_lamy', 'associate_presbytery', 'attorney_general', 'baby_christened', 'below_zero', 'bentley_belleville', 'bids_fair', 'big_jim', 'bishop_machebeuf', 'black_flies', 'black_walnut', 'blade_grass', 'both_sides', 'boundary_line', 'british_columbia', 'burying_ground', 'bushels_wheat', 'cab_hire', 'cabin_luggage', 'canal_boats', 'cape_breton', 'captain_ferrier', 'captain_orlebar', 'captain_thorndike', 'carrothers_lisbellaw', 'castor_oil', 'catharine_parr', 'cedar_swamp', 'cents_per', 'charles_haszard', 'circuit_court', 'civil_war', 'colonel_chavez', 'colonel_myers', 'colonial_building', 'commodious_habitations', 'commodious_seats', 'cooking_utensils', 'copy_roughing', 'corrugated_iron', 'county_durham', 'crazy_ann', 'crushing_cylinder', 'cup_tea', 'dark_cloud', 'degrees_below', 'delegate_congress', 'deo_gratias', 'dick_wooten', 'difference_between', 'different_parts', 'dining_room', 'discharged_soldiers', 'doctor_russ', 'don_santiago', 'dona_juanita', 'dona_nieves', 'dorothy_chance', 'earl_bathurst', 'eastern_townships', 'edward_iii', 'equally_cheap', 'erie_canal', 'everything_else', 'falls_niagara', 'father_gasparri', 'father_massa', 'father_pinto', 'father_stephan', 'father_zahm', 'fee_simple', 'feet_diameter', 'feet_wide', 'female_servants', 'fermanagh_enniskillen', 'ferry_boat', 'few_lines', 'flora_lyndsay', 'flour_mills', 'fort_wellington', 'francis_xavier', 'fresh_air', 'friday_afternoon', 'fruit_trees', 'garden_stuff', 'general_carleton', 'general_manager', 'geoffrey_moncton', 'get_rid', 'god_bless', 'good_bye', 'greater_part', 'grist_mill', 'grist_mills', 'groups_flowers', 'half_dozen', 'half_hour', 'henry_viii', 'hermana_dolores', 'high_altitude', 'holy_trinity', 'house_assembly', 'hudson_river', 'human_beings', 'human_nature', 'humble_servant', 'huron_tract', 'imported_goods', 'impulse_run', 'indian_corn', 'indian_tribes', 'industrial_school', 'inland_navigation', 'intense_heat', 'james_copeland', 'james_topham', 'jeanie_burns', 'jim_pockets', 'john_kirby', 'joseph_carrothers', 'kansas_city', 'katie_vickers', 'kind_regards', 'kindest_love', 'kindest_regards', 'king_gurmond', 'kit_carson', 'lake_erie', 'lake_michigan', 'lake_ontario', 'lake_simcoe', 'lamy_junction', 'land_grabbers', 'las_vegas', 'letters_introduction', 'literary_garland', 'long_creek', 'lord_castlereagh', 'lord_hawkesbury', 'lord_moira', 'lord_supper', 'lower_canada', 'lower_province', 'maple_tree', 'margaret_ellin', 'maria_tully', 'mark_hurdlestone', 'martha_april', 'martha_february', 'martha_january', 'martha_march', 'martha_november', 'martha_october', 'martha_september', 'mary_josephine', 'mary_teresa', 'matrimonial_speculations', 'metallic_fluid', 'mexican_smart', 'miss_clares', 'miss_muncey', 'miss_strickland', 'moderate_rate', 'moodie_unites', 'mother_regina', 'mrs_dougall', 'mrs_fitzgerald', 'mrs_forsyth', 'mrs_gall', 'mrs_montgomery', 'mrs_oldfield', 'murray_harbour', 'musical_programme', 'narrow_escape', 'nathaniel_carrothers', 'native_population', 'new_mexico', 'new_orleans', 'new_york', 'nicola_lake', 'nine_tenths', 'north_douro', 'nova_scotia', 'obedient_servant', 'omnibus_bill', 'opposite_side', 'orphan_asylum', 'orphan_girls', 'otero_chavez', 'parliament_assembled', 'parr_traill', 'partly_cleared', 'patriotic_songs', 'peep_day', 'per_acre', 'per_annum', 'per_bushel', 'per_cent', 'per_cwt', 'per_pound', 'petition_david', 'pleasantly_situated', 'political_principles', 'poll_book', 'pork_curing', 'post_office', 'postmarked_london', 'pounds_sterling', 'prairie_dogs', 'prairie_meadows', 'presbyterian_hospital', 'prince_edward', 'principal_streets', 'printing_offices', 'private_conveyance', 'provincial_parliament', 'queens_england', 'rail_road', 'rainy_season', 'real_estate', 'reasonably_expect', 'religious_instruction', 'returning_officer', 'rev_archbishop', 'rev_bishop', 'rev_gasparri', 'rev_lamy', 'richard_bentley', 'rio_grande', 'roman_catholic', 'roman_catholics', 'rye_grass', 'safe_arrival', 'saint_patrick', 'samuel_street', 'san_felipe', 'san_francisco', 'san_marcial', 'san_miguel', 'santa_catalina', 'santa_claus', 'santa_trail', 'santiago_baca', 'sarah_martha', 'school_warrants', 'sea_ports', 'secretary_state', 'serjeant_arms', 'seventy_five', 'shortly_after', 'signed_alexander', 'signed_sandy', 'silver_coins', 'simpson_rest', 'sister_augustine', 'sister_blandina', 'sister_catherine', 'sister_justina', 'sister_pauline', 'sisters_charity', 'sisters_loretto', 'sit_down', 'sixty_seventy', 'solid_silver', 'spit_spit', 'steam_boat', 'steam_boats', 'steam_engine', 'steerage_passengers', 'strange_boulton', 'strong_drinks', 'susanna_moodie', 'sweet_tempered', 'talbot_settlement', 'taylor_preached', 'tea_coffee', 'teacher_salary', 'ten_minutes', 'thank_god', 'thomas_ward', 'thou_shalt', 'thousand_inhabitants', 'three_fourths', 'tour_through', 'trinidad_colorado', 'truly_susanna', 'twelve_fourteen', 'typhoid_fever', 'united_states', 'universal_suffrage', 'upper_canada', 'upper_province', 'van_buren', 'vicar_general', 'vigilant_club', 'vigilant_committee', 'war_whoop', 'warm_gloves', 'wee_bit', 'whooping_cough', 'whose_descendants', 'wild_animals', 'wild_flowers', 'william_hamilton', 'william_sampson', 'years_ago', 'yoke_oxen', 'young_ladies', 'yours_truly']\n"
     ]
    }
   ],
   "source": [
    "# See results of bigrams function\n",
    "bigrams = [] # Create list\n",
    "for item in data_words: # For each item (i.e., letter) in data words\n",
    "    bigrams.append([b for b in bigram[item] if b.count('_') == 1]) # add 2-grams to new list\n",
    "bigrams = list(np.concatenate(bigrams)) # flatten list\n",
    "bigrams = list(dict.fromkeys(bigrams)) # take unique values\n",
    "print(sorted(bigrams)) # print in alphabetical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absolutely_necessary', 'air_exercises', 'albany_buffalo', 'ancient_civilization', 'angels_guard', 'annual_pass', 'archbishop_lamy', 'associate_presbytery', 'attorney_general', 'baby_christened', 'below_zero', 'bids_fair', 'big_jim', 'bishop_durango', 'black_flies', 'blade_grass', 'both_sides', 'boundary_line', 'british_columbia', 'burying_ground', 'bushels_wheat', 'cab_hire', 'cabin_luggage', 'canal_boats', 'captain_orlebar', 'captain_thorndike', 'carrothers_lisbellaw', 'cedar_swamp', 'cents_bushel', 'cents_per', 'charles_haszard', 'christmas_tree', 'circuit_court', 'civil_war', 'colonel_chavez', 'colonel_myers', 'colonial_building', 'commodious_habitations', 'commodious_seats', 'cooking_utensils', 'copy_roughing', 'county_durham', 'crazy_ann', 'crushing_cylinder', 'cup_tea', 'dark_cloud', 'days_ago', 'dear_bentley', 'dearest_catherine', 'degrees_below', 'delegate_congress', 'dick_wooten', 'difference_between', 'different_parts', 'dining_room', 'discharged_soldiers', 'doctor_russ', 'dollars_acre', 'don_santiago', 'dona_juanita', 'earl_bathurst', 'eastern_townships', 'edward_iii', 'equally_cheap', 'erie_canal', 'everything_else', 'falls_niagara', 'father_gasparri', 'father_pinto', 'father_stephan', 'fee_simple', 'feet_diameter', 'feet_wide', 'female_servants', 'fermanagh_enniskillen', 'ferry_boat', 'few_lines', 'few_minutes', 'flora_lyndsay', 'flour_mills', 'fort_wellington', 'francis_xavier', 'fresh_air', 'friday_afternoon', 'fruit_trees', 'garden_stuff', 'general_carleton', 'general_jackson', 'general_manager', 'get_rid', 'god_bless', 'good_bye', 'got_rid', 'greater_part', 'grist_mill', 'grist_mills', 'groups_flowers', 'half_dozen', 'half_hour', 'henry_viii', 'hermana_dolores', 'high_altitude', 'holy_trinity', 'house_assembly', 'hudson_river', 'human_beings', 'human_nature', 'humble_servant', 'huron_tract', 'imported_goods', 'impulse_run', 'indian_corn', 'indian_tribes', 'industrial_school', 'inland_navigation', 'intense_heat', 'james_copeland', 'james_topham', 'john_kirby', 'joseph_carrothers', 'kansas_city', 'katie_vickers', 'kind_regards', 'kindest_love', 'kindest_regards', 'king_gurmond', 'kit_carson', 'lake_erie', 'lake_michigan', 'lake_ontario', 'lake_simcoe', 'lamy_junction', 'letters_introduction', 'literary_garland', 'long_creek', 'lord_castlereagh', 'lord_hawkesbury', 'lord_moira', 'lord_supper', 'loretto_sisters', 'lower_canada', 'lower_province', 'maple_tree', 'maria_tully', 'mark_hurdlestone', 'mary_josephine', 'matrimonial_speculations', 'metallic_fluid', 'mexican_smart', 'miss_armstrong', 'miss_clares', 'miss_forsyth', 'miss_fox', 'miss_strickland', 'moderate_rate', 'moodie_unites', 'mother_josephine', 'mrs_dougall', 'mrs_fitzgerald', 'mrs_forsyth', 'mrs_gall', 'mrs_lawson', 'mrs_montgomery', 'mrs_murney', 'mrs_oldfield', 'mrs_vickers', 'murray_harbour', 'musical_programme', 'narrow_escape', 'nathaniel_carrothers', 'native_population', 'new_brunswick', 'new_mexico', 'new_orleans', 'new_york', 'nine_tenths', 'north_douro', 'obedient_servant', 'omnibus_bill', 'opposite_side', 'orphan_asylum', 'orphan_girls', 'otero_chavez', 'parliament_assembled', 'partly_cleared', 'peep_day', 'per_acre', 'per_barrel', 'per_bushel', 'per_cent', 'per_cwt', 'per_gallon', 'per_pound', 'petition_david', 'pleasantly_situated', 'political_principles', 'poll_book', 'pork_curing', 'post_office', 'postmarked_london', 'pounds_sterling', 'prairie_dogs', 'prairie_meadows', 'presbyterian_hospital', 'prince_edward', 'principal_streets', 'printing_offices', 'private_conveyance', 'provincial_parliament', 'queens_england', 'rail_road', 'rainy_season', 'rainy_weather', 'real_estate', 'reasonably_expect', 'religious_instruction', 'returning_officer', 'rev_archbishop', 'rev_bishop', 'rev_gasparri', 'rev_lamy', 'richard_bentley', 'roman_catholic', 'roman_catholics', 'rye_grass', 'safe_arrival', 'saint_patrick', 'samuel_street', 'san_felipe', 'san_francisco', 'santa_claus', 'santa_trail', 'sarah_martha', 'school_warrants', 'sea_ports', 'secretary_state', 'serjeant_arms', 'seventy_five', 'shortly_after', 'signed_alexander', 'signed_sandy', 'silver_coins', 'simpson_rest', 'sister_augustine', 'sister_blandina', 'sister_catherine', 'sister_dolores', 'sister_eulalia', 'sister_gabriella', 'sister_louise', 'sister_pauline', 'sisters_charity', 'sisters_loretto', 'sit_down', 'sixty_seventy', 'solid_silver', 'steam_boat', 'steam_boats', 'steam_engine', 'steerage_passengers', 'strange_boulton', 'strong_drinks', 'susanna_moodie', 'susanna_robb', 'sweet_tempered', 'talbot_settlement', 'taylor_preached', 'tea_coffee', 'teacher_salary', 'ten_minutes', 'ten_twelve', 'thank_god', 'thomas_ward', 'thousand_inhabitants', 'tour_through', 'trinidad_colorado', 'truly_susanna', 'twelve_fourteen', 'typhoid_fever', 'united_states', 'upper_canada', 'upper_province', 'vicar_general', 'vigilant_club', 'vigilant_committee', 'war_whoop', 'warm_gloves', 'wee_bit', 'weeks_ago', 'western_states', 'whose_descendants', 'wild_animals', 'wild_flowers', 'william_hamilton', 'william_sampson', 'years_ago', 'yoke_oxen', 'young_ladies', 'yours_truly']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# See results of trigrams function\n",
    "\n",
    "trigrams1 = [] # Create list\n",
    "for item in data_words: # For each item (i.e., chunk) in data words\n",
    "    trigrams1.append([b for b in trigram[item] if b.count('_') == 1]) # add 2-grams to new list\n",
    "trigrams1 = list(np.concatenate(trigrams1)) # flatten list\n",
    "trigrams1 = list(dict.fromkeys(trigrams1)) # take unique values\n",
    "print(sorted(trigrams1)) # print in alphabetical order\n",
    "\n",
    "trigrams2 = [] # Create list\n",
    "for item in data_words: # For each item (i.e., chunk) in data words\n",
    "    trigrams2.append([b for b in trigram[item] if b.count('_') == 2]) # add 2-grams to new list\n",
    "trigrams2 = list(np.concatenate(trigrams2)) # flatten list\n",
    "trigrams2 = list(dict.fromkeys(trigrams2)) # take unique values\n",
    "print(sorted(trigrams2)) # print in alphabetical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5631"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 19th century stopwords\n",
    "stop_words = pd.read_csv(\"Jockers_19thCenturyStops.csv\")\n",
    "stop_words = stop_words['word'].values.astype(str).tolist()\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'VERB']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Form Trigrams\n",
    "data_words_trigrams = make_trigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy 'en' model\n",
    "# python3 -m spacy download en_core_web_md\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sister', 'anxiety', 'brother', 'outcast', 'journal', 'deal', 'incident', 'journey', 'land', 'journal', 'mother', 'letter', 'child', 'mother', 'mother', 'letter', 'sacrifice', 'hiding', 'feeling', 'merit', 'find', 'map', 'island', 'destination', 'pupil', 'teacher']]\n"
     ]
    }
   ],
   "source": [
    "# Do lemmatization keeping only nouns\n",
    "data_lemmatizedNouns = lemmatization(data_words_bigrams, allowed_postags=['NOUN'])\n",
    "print(data_lemmatizedNouns[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['read', 'save', 'propose', 'keep', 'occur', 'happening', 'consign', 'missione', 'attend', 'need', 'thrill', 'lose', 'conclude', 'leave', 'cause']]\n"
     ]
    }
   ],
   "source": [
    "# Do lemmatization keeping only verbs\n",
    "data_lemmatizedVerbs = lemmatization(data_words_bigrams, allowed_postags=['VERB'])\n",
    "print(data_lemmatizedVerbs[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sister', 'read', 'anxiety', 'save', 'brother', 'outcast', 'journal', 'propose', 'keep', 'deal', 'incident', 'occur', 'journey', 'happening', 'land', 'consign', 'journal', 'mother', 'letter', 'child', 'missione', 'mother', 'attend', 'need', 'mother', 'letter', 'thrill', 'sacrifice', 'hiding', 'feeling', 'lose', 'merit', 'find', 'map', 'island', 'conclude', 'destination', 'leave', 'pupil', 'cause', 'teacher']]\n"
     ]
    }
   ],
   "source": [
    "# Do lemmatization keeping only noun, verb\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'VERB'])\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"20240220_PhD_ChkLem-N.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(data_lemmatizedNouns, fp)\n",
    "\n",
    "with open(\"20240220_PhD_ChkLem-V.txt.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(data_lemmatizedVerbs, fp)\n",
    "\n",
    "with open(\"20240220_PhD_ChkLem-NV.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(data_lemmatized, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
